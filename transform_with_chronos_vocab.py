"""
Transform files using existing Chronos vocabularies instead of creating new ones.
This script reuses vocabularies generated by process_chronos_dataset.py (chronos_*.fvocab).
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
from sklearn.preprocessing import StandardScaler
import joblib
from utils.discretisize import simple_discretize, adaptative_bins_discretize, load_float_vocab
from utils.token_based import TokenBasedTokenizer

# Base paths
base_data_files = 'data/datasets/'
base_data_file_out = 'data/outputs/chronos_vocab/'

# Dataset files to process
list_of_files = ['ETTh1', 'ETTh2', 'ETTm1', 'ETTm2']

# Special tokens configuration
def get_special_tokens(N_samples):
    """Get special tokens for given N_samples."""
    return {'<PAD>': N_samples - 1, '<EBOS>': N_samples}


def find_chronos_vocab_files():
    """Find all existing chronos vocabulary files and organize by configuration."""
    float_vocab_dir = Path('float_vocab')
    model_dir = Path('model')
    
    if not float_vocab_dir.exists():
        raise FileNotFoundError(f"Directory 'float_vocab' not found!")
    
    # Find all chronos .fvocab files
    vocab_files = list(float_vocab_dir.glob('chronos_*.fvocab'))
    
    if not vocab_files:
        raise FileNotFoundError(
            "No chronos_*.fvocab files found! "
            "Please run process_chronos_dataset.py first to generate vocabularies."
        )
    
    print(f"\n{'='*60}")
    print(f"FOUND CHRONOS VOCABULARIES")
    print(f"{'='*60}")
    
    vocab_info = {}
    configs_set = set()  # Track unique (N_samples, vocab_size) combinations
    
    for vocab_file in vocab_files:
        vocab_name = vocab_file.stem
        # Parse: chronos_N{N}_vocab{V}_target_{disc_type}
        parts = vocab_name.split('_')
        
        try:
            n_samples = int(parts[1][1:])  # N22 -> 22
            vocab_size = int(parts[2][5:])  # vocab1373 -> 1373
            disc_type = parts[-1]  # simple or adaptative
            
            key = (n_samples, vocab_size, disc_type)
            vocab_info[key] = {
                'vocab_file': vocab_file.name,  # Just the filename, not full path
                'model_file': str(model_dir / f"{vocab_name}.model")
            }
            
            configs_set.add((n_samples, vocab_size))
            
            print(f"  ✓ N={n_samples}, Vocab={vocab_size}, Type={disc_type}")
        except (IndexError, ValueError):
            print(f"  ⚠ Skipping malformed vocab: {vocab_name}")
    
    print(f"\n  Total: {len(vocab_info)} vocabularies found")
    print(f"  Unique configurations: {len(configs_set)}")
    
    # Return vocab_info and sorted list of unique configs
    configs_list = sorted(list(configs_set))
    return vocab_info, configs_list


def tokenize_column_with_chronos_vocab(data_series, N_samples, total_vocab_size, vocab_info, column_name):
    """
    Tokenize a column using existing Chronos vocabularies.
    
    Parameters:
    -----------
    data_series : pd.Series
        Numeric data to tokenize (already standardized)
    N_samples : int
        Number of discretization bins
    total_vocab_size : int
        Total vocabulary size after BPE
    vocab_info : dict
        Dictionary with vocabulary file paths
    column_name : str
        Name of the column being processed
    
    Returns:
    --------
    dict with 'simple' and 'adaptative' results
    """
    special_tokens = get_special_tokens(N_samples)
    clean_data = data_series.dropna()
    
    if len(clean_data) < 10:
        return None
    
    results = {}
    
    # Process both simple and adaptative
    for disc_type in ['simple', 'adaptative']:
        key = (N_samples, total_vocab_size, disc_type)
        
        if key not in vocab_info:
            print(f"    ⚠ Warning: No {disc_type} vocab for N={N_samples}, Vocab={total_vocab_size}")
            continue
        
        vocab_file = vocab_info[key]['vocab_file']
        model_file = vocab_info[key]['model_file']
        
        try:
            # Load existing vocabulary (bin edges)
            bin_edges = load_float_vocab(vocab_file)
            
            # Discretize using existing bins
            # We need to manually bin the data using the loaded edges
            discretized = np.digitize(clean_data.values, bin_edges[1:-1])  # Exclude outer boundaries
            discretized = np.clip(discretized, 0, N_samples - 1)
            
            # Load tokenizer model
            if not os.path.exists(model_file):
                print(f"    ⚠ Warning: Model file not found: {model_file}")
                continue
            
            tokenizer = TokenBasedTokenizer(N_samples, vocab_file, special_tokens=special_tokens)
            tokenizer.load(model_file)
            
            # Encode with BPE
            encoded = tokenizer.encode(discretized.tolist())
            
            results[disc_type] = {
                'encoded': encoded,
                'original_size': len(discretized),
                'compressed_size': len(encoded),
                'compression_rate': len(discretized) / len(encoded) if len(encoded) > 0 else 0
            }
            
        except Exception as e:
            print(f"    ⚠ Error processing {disc_type}: {e}")
    
    return results if results else None


def process_file_with_chronos_vocab(file_path, file_name, vocab_info, configs_list):
    """Process a single file using Chronos vocabularies."""
    print(f"\n{'='*60}")
    print(f"PROCESSING: {file_name}")
    print(f"{'='*60}")
    
    df = pd.read_csv(file_path)
    list_columns = [col for col in df.columns if col != 'date']
    
    # Create output directory
    os.makedirs(base_data_file_out, exist_ok=True)
    
    # Create directory for scalers
    scaler_dir = 'scalers/chronos_vocab'
    os.makedirs(scaler_dir, exist_ok=True)
    
    # Process each configuration dynamically extracted from vocab files
    for N_samples, total_vocab_size in configs_list:
        print(f"\n  Configuration: N={N_samples}, Vocab={total_vocab_size}")
        special_tokens = get_special_tokens(N_samples)
        
        # Initialize dataframes for this config
        tokenized_dfs = {
            'simple': pd.DataFrame(),
            'adaptative': pd.DataFrame(),
        }
        
        for column in list_columns:
            print(f"    Processing column: {column}")
            
            y_value = df[column]
            
            # Standardize
            scaler = StandardScaler()
            y_scaled = pd.Series(
                scaler.fit_transform(y_value.to_frame()).ravel(),
                index=y_value.index,
                name=column
            )
            
            # Save scaler
            scaler_path = f"{scaler_dir}/{file_name}_N{N_samples}_column_{column}.pkl"
            joblib.dump(scaler, scaler_path)
            
            # Tokenize using Chronos vocab
            results = tokenize_column_with_chronos_vocab(
                y_scaled, N_samples, total_vocab_size, vocab_info, column
            )
            
            if results:
                for disc_type in ['simple', 'adaptative']:
                    if disc_type in results:
                        tokenized_dfs[disc_type][column] = pd.Series(results[disc_type]['encoded'])
                        print(f"      {disc_type.capitalize()}: {results[disc_type]['original_size']} → "
                              f"{results[disc_type]['compressed_size']} tokens "
                              f"(compression: {results[disc_type]['compression_rate']:.2f}x)")
        
        # Save tokenized dataframes
        for disc_type in ['simple', 'adaptative']:
            if not tokenized_dfs[disc_type].empty:
                output_filename = f"{file_name}_chronos_vocab_N{N_samples}_V{total_vocab_size}_{disc_type}.csv"
                output_path = os.path.join(base_data_file_out, output_filename)
                
                # Fill NaN with PAD token
                df_tokenized = tokenized_dfs[disc_type].fillna(special_tokens['<PAD>'])
                df_tokenized.to_csv(output_path, index=False)
                print(f"    ✓ Saved: {output_filename}")


def main():
    """Main execution flow."""
    print(f"{'='*60}")
    print(f"TRANSFORM FILES WITH CHRONOS VOCABULARIES")
    print(f"{'='*60}")
    print(f"\nThis script uses existing chronos_*.fvocab files")
    print(f"instead of creating new vocabularies.\n")
    
    # Find existing Chronos vocabularies and extract configurations
    try:
        vocab_info, configs_list = find_chronos_vocab_files()
        print(f"\n  Configurations to process: {configs_list}")
    except FileNotFoundError as e:
        print(f"\n❌ Error: {e}")
        return
    
    # Process each file
    for file_name in list_of_files:
        file_path = f"{base_data_files}{file_name}.csv"
        
        if not os.path.exists(file_path):
            print(f"\n⚠ Warning: File not found: {file_path}")
            continue
        
        process_file_with_chronos_vocab(file_path, file_name, vocab_info, configs_list)
    
    print(f"\n{'='*60}")
    print(f"PROCESSING COMPLETE")
    print(f"{'='*60}")
    print(f"\nOutput files saved to: {base_data_file_out}")


if __name__ == "__main__":
    main()
